---
layout: post
title: Aprendizaje por refuerzo
description: Aprendizaje por refuerzo, de los métodos tabulares al aperendizaje profundo
image: assets/images/refuerzo.jpg
---

## Libro de texto

[Reinforcement Learning: An
Introduction](https://drive.google.com/file/d/1opPSz5AZ_kVa1uWOdOiveNiBFiEOHjkG/view)
de Richard S. Sutton and Andrew G. Barto. La biblia del aprendizaje por refuerzo (y más
esta segunda edición que publicará el MIT Press en noviembre de 2018). Publicado bajo un
acuerdo de *open access*, por lo que el libro está disponible en forma gratuita por los autores.

El capítulo 6 y el capítulo 9 son esenciales para los conceptos básicos de RL.



## Otro material

1. [Pequeño resumen de aprendizaje por refuerzo](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html)

2. [Algoritmos basados engradiente de política](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html)

3. [Presentación sobre *Alpha Zero*](assets/docs/alphazero.pdf)


## Ejercicios y proyectos de evaluación para el tópico de aprendizaje por refuerzo

Para evaluar las competencias adquiridas sobre aprendizaje por refuerzo, hemos dividido la evaluacion de ésta unidad en trés partes diferentes:

1. Ejercicios para comprender los algoritmos básicos. Estos ejercicios son 3 ejemplos básicos que vienen el en libro de Sutton con pequeñas modificaciones propuestas como ejercicios, los cuales son:

  - *Windy Grid World*
  - *Cliff Walking*
  - *Mountain Car*
  
2. Desarrollar el modelo de la veintiuna simplificada utilizada en el tópico de programación dinámica y comparr el uso de un método como SARSA o QL y sus ventajas y desventajas respecto a la programación dinámica.

3. Un modelo de RL para un juego en particular (como pacman, spaceinvaders, o el control de un acrobot, un robot que camina, etc., basados en los modelos existentes en *openai gym*.

## Ejercicios desarrollados por estudiante para su evaluación

| Estudiante | Ejercicios Libro de Sutton | Veintiuna | *open IA* |
| ------     | -------------------------- | --------- | --------- |                                                                                                                         |
| Belen      | [Ejercicios](https://github.com/chasil7/topicosIA/tree/master/Aprendizaje_Refuerzo) | [veintiuna](https://github.com/chasil7/topicosIA/tree/master/Aprendizaje_Refuerzo/Black_Jack)| [Acrobot](https://github.com/chasil7/topicosIA/tree/master/Aprendizaje_Refuerzo/Acrobot) |
| Adrián     | pendiente  | pendiente | pendiente |
| Fernando   | [Ejercicios](https://github.com/fsr313/TADIA/tree/master/RL)| [veintiuna](https://github.com/fsr313/TADIA/blob/master/RL/Blackjack.ipynb)| [Acrobot](https://github.com/fsr313/TADIA/blob/master/RL/Gym%20Acrobot%20JL.ipynb) |
| Ivan       | [Ejercicios](https://rexemin.github.io/Topicos-IA-UNISON/2018/11/30/ejercicios-sutton-barto.html)   | [Veintiuna](https://rexemin.github.io/Topicos-IA-UNISON/2018/12/03/revancha-21.html) | [Acrobot](https://rexemin.github.io/Topicos-IA-UNISON/2018/12/09/acrobot.html) |
| Ricardo    | [Ejercicios](https://github.com/RicardoHE97/TopicosIA-Unison/tree/master/reinforcement-learning-an-introduction) | [veintiuna](https://github.com/RicardoHE97/TopicosIA-Unison/blob/master/Q_learning_BlackJack/Blackjack%20-%20Q-Learning.ipynb) | [Acrobot](https://github.com/RicardoHE97/TopicosIA-Unison/tree/master/Acrobot-v1) |
| Giovanni   | [Ejercicios](https://github.com/LuiGiovanni/Topicos_de_IA/tree/master/RL)  | [veintiuna](https://github.com/LuiGiovanni/Topicos_de_IA/tree/master/RL/Blackjack) | [Acrobot](https://github.com/LuiGiovanni/Topicos_de_IA/tree/master/RL/Acrobot) y [Pacman](https://github.com/LuiGiovanni/Topicos_de_IA/tree/master/PacmanDQN)


Para esta unidad vamos a realizar una evaluación democrática en varios pasos. En
un primer paso, vamos a organizar a todos los trabajos de los compañeros en
orden, donde el 1 es el mejor trabajo y el 7 el menos bueno. Junto a la
evaluación, vamos a incluir una opción en la que se considera si el compañero
aprobó no no la evaluación. Sobre los aprobados, haremos una dinámica en clase
para asignar las calificaciones.



